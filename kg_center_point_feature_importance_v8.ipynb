{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        os.path.join(dirname, filename)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ca56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34739343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # [NEW] Memory Optimization: Limit max samples per video/pair\n",
    "    train_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\"\n",
    "    test_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    model_path = \"models\"\n",
    "    model_name_xgb_base = \"xgboost_unified_base\"\n",
    "    \n",
    "    n_splits = 3\n",
    "    \n",
    "    # A high confidence threshold to ensure only high-quality pseudo-labels are used.\n",
    "    CONFIDENCE_THRESHOLD = 0.90\n",
    "\n",
    "    # Define a canonical set of body parts to be used for all feature generation.\n",
    "    CANONICAL_BODY_PARTS = [\n",
    "        'nose', 'ear_left', 'ear_right', 'body_center', 'tail_base', \n",
    "        'spine_middle', 'tail_tip'\n",
    "    ]\n",
    "\n",
    "    # XGBoost parameters\n",
    "    xgb_params = {\n",
    "        'n_estimators': 300,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist', # Changed to 'hist' for CPU/Mac compatibility\n",
    "        'random_state': 42,\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "\n",
    "    # Define label groups for rare behaviors\n",
    "    OTHERS1_ACTIONS = [\n",
    "        'climb', 'dominancegroom', 'allogroom', 'attemptmount', \n",
    "        'reciprocalsniff', 'dominancemount', 'disengage', 'defend'\n",
    "    ]\n",
    "    \n",
    "    OTHERS2_ACTIONS = [\n",
    "        'ejaculate', 'dominance', 'freeze', 'huddle', 'shepherd', \n",
    "        'genitalgroom', 'run',  'rest', 'flinch', \n",
    "        'tussle', 'biteobject', 'exploreobject', 'follow'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directories\n",
    "os.makedirs(f\"{CFG.model_path}/{CFG.model_name_xgb_base}\", exist_ok=True)\n",
    "os.makedirs(f\"{CFG.model_path}/{CFG.model_name_xgb_augmented}\", exist_ok=True)\n",
    "\n",
    "drop_body_parts =  [\n",
    "    'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "    'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "    'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11969cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data Loading and Feature Engineering Functions\n",
    "# =============================================================================\n",
    "def safe_rolling(series, window, func, min_periods=None):\n",
    "    if min_periods is None:\n",
    "        min_periods = max(1, window // 4)\n",
    "    return series.rolling(window, min_periods=min_periods, center=True).apply(func, raw=True)\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def compute_egocentric_transform(mouse_df, body_parts, target_points=None):\n",
    "    \"\"\"\n",
    "    Transforms coordinates to an egocentric frame based on body_center and tail_base.\n",
    "    Origin: body_center\n",
    "    Y-axis: vector from tail_base to body_center (spine points UP)\n",
    "    \"\"\"\n",
    "    if 'body_center' not in mouse_df.columns or 'tail_base' not in mouse_df.columns:\n",
    "        return None\n",
    "    \n",
    "    # 1. Translation: Center at body_center\n",
    "    origin_x = mouse_df['body_center']['x']\n",
    "    origin_y = mouse_df['body_center']['y']\n",
    "    \n",
    "    # 2. Rotation: Align spine (tail_base -> body_center) with Y-axis\n",
    "    # Vector from tail to center\n",
    "    spine_x = mouse_df['body_center']['x'] - mouse_df['tail_base']['x']\n",
    "    spine_y = mouse_df['body_center']['y'] - mouse_df['tail_base']['y']\n",
    "    \n",
    "    # Angle of spine relative to Y-axis (0, 1)\n",
    "    # We want to rotate so that (spine_x, spine_y) becomes (0, +mag)\n",
    "    # Current angle of spine in global coords\n",
    "    theta = np.arctan2(spine_y, spine_x)\n",
    "    # We want this to be pi/2 (90 degrees, pointing up)\n",
    "    # So we rotate by (pi/2 - theta)\n",
    "    rotation_angle = np.pi/2 - theta\n",
    "    \n",
    "    cos_a = np.cos(rotation_angle)\n",
    "    sin_a = np.sin(rotation_angle)\n",
    "    \n",
    "    ego_data = {}\n",
    "    \n",
    "    # Transform own body parts\n",
    "    for part in body_parts:\n",
    "        if part in mouse_df.columns:\n",
    "            # Translate\n",
    "            dx = mouse_df[part]['x'] - origin_x\n",
    "            dy = mouse_df[part]['y'] - origin_y\n",
    "            # Rotate\n",
    "            ego_data[f'ego_{part}_x'] = dx * cos_a - dy * sin_a\n",
    "            ego_data[f'ego_{part}_y'] = dx * sin_a + dy * cos_a\n",
    "            \n",
    "    # Transform target points (e.g., other mouse's body parts) if provided\n",
    "    if target_points is not None:\n",
    "        for col_name, (tx, ty) in target_points.items():\n",
    "            dx = tx - origin_x\n",
    "            dy = ty - origin_y\n",
    "            ego_data[f'ego_target_{col_name}_x'] = dx * cos_a - dy * sin_a\n",
    "            ego_data[f'ego_target_{col_name}_y'] = dx * sin_a + dy * cos_a\n",
    "            \n",
    "    return pd.DataFrame(ego_data, index=mouse_df.index)\n",
    "\n",
    "\n",
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    if 'frames_per_second' in meta_df.columns and pd.notnull(meta_df['frames_per_second']).any():\n",
    "        return float(meta_df['frames_per_second'].iloc[0])\n",
    "    vid = meta_df['video_id'].iloc[0]\n",
    "    return float(fallback_lookup.get(vid, default_fps))\n",
    "\n",
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    acc_x = vel_x.diff()\n",
    "    acc_y = vel_y.diff()\n",
    "    cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "    vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)\n",
    "    for w in [25, 50, 75]:\n",
    "        ws = _scale(w, fps)\n",
    "        X[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    return X\n",
    "\n",
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    scales = [20, 40, 60, 80]\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n",
    "            X[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).std()\n",
    "    if len(scales) >= 2 and f'sp_m{scales[0]}' in X.columns and f'sp_m{scales[-1]}' in X.columns:\n",
    "        X['sp_ratio'] = X[f'sp_m{scales[0]}'] / (X[f'sp_m{scales[-1]}'] + 1e-6)\n",
    "    return X\n",
    "\n",
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    w_ma = _scale(15, fps)\n",
    "    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n",
    "    try:\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "        for window in [20, 40, 60, 80]:\n",
    "            ws = _scale(window, fps)\n",
    "            if len(speed_states) >= ws:\n",
    "                for state in [0, 1, 2, 3]:\n",
    "                    X[f's{state}_{window}'] = ((speed_states == state).astype(float).rolling(ws, min_periods=max(1, ws // 5)).mean())\n",
    "                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "                X[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(center_x) >= ws:\n",
    "            X[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "            X[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "    for span in [30, 60, 120]:\n",
    "        s = _scale(span, fps)\n",
    "        X[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        X[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n",
    "    return X\n",
    "\n",
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "    for window in [30, 60]:\n",
    "        ws = _scale(window, fps)\n",
    "        X[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "        X[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "    approach = -rel_dist.diff()\n",
    "    chase = approach * B_lead\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "        B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "        X[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d61c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_single(single_mouse, body_parts_tracked, fps):\n",
    "    available_body_parts = single_mouse.columns.get_level_values(0)\n",
    "    X = pd.DataFrame({f\"{p1}+{p2}\": np.square(single_mouse[p1] - single_mouse[p2]).sum(axis=1, skipna=False) for p1, p2 in itertools.combinations(body_parts_tracked, 2) if p1 in available_body_parts and p2 in available_body_parts})\n",
    "    X = X.reindex(columns=[f\"{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n",
    "    if all(p in single_mouse.columns for p in ['ear_left', 'ear_right', 'tail_base']):\n",
    "        lag = _scale(10, fps)\n",
    "        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag)\n",
    "        speeds = pd.DataFrame({'sp_lf': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False), 'sp_rt': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False), 'sp_lf2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False), 'sp_rt2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False)})\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "    if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        v1 = single_mouse['nose'] - single_mouse['body_center']\n",
    "        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n",
    "        X['body_ang'] = (v1['x'] * v2['x'] + v1['y'] * v2['y']) / (np.sqrt(v1['x']**2 + v1['y']**2) * np.sqrt(v2['x']**2 + v2['y']**2) + 1e-6)\n",
    "    if 'body_center' in available_body_parts:\n",
    "        cx = single_mouse['body_center']['x']\n",
    "        cy = single_mouse['body_center']['y']\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'cx_m{w}'] = cx.rolling(ws, **roll).mean()\n",
    "            X[f'cy_m{w}'] = cy.rolling(ws, **roll).mean()\n",
    "            X[f'cx_s{w}'] = cx.rolling(ws, **roll).std()\n",
    "            X[f'cy_s{w}'] = cy.rolling(ws, **roll).std()\n",
    "            X[f'x_rng{w}'] = cx.rolling(ws, **roll).max() - cx.rolling(ws, **roll).min()\n",
    "            X[f'y_rng{w}'] = cy.rolling(ws, **roll).max() - cy.rolling(ws, **roll).min()\n",
    "            X[f'disp{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).sum()**2 + cy.diff().rolling(ws, min_periods=1).sum()**2)\n",
    "            X[f'act{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).var() + cy.diff().rolling(ws, min_periods=1).var())\n",
    "        X = add_curvature_features(X, cx, cy, fps)\n",
    "        X = add_multiscale_features(X, cx, cy, fps)\n",
    "        X = add_state_features(X, cx, cy, fps)\n",
    "        X = add_longrange_features(X, cx, cy, fps)\n",
    "    if all(p in available_body_parts for p in ['nose', 'tail_base']):\n",
    "        nt_dist = np.sqrt((single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 + (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nt_lg{lag}'] = nt_dist.shift(l)\n",
    "            X[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l)\n",
    "    if all(p in available_body_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_d = np.sqrt((single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 + (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2)\n",
    "        for off in [-30, -20, -10, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'ear_o{off}'] = ear_d.shift(-o)\n",
    "        w = _scale(30, fps)\n",
    "        X['ear_con'] = ear_d.rolling(w, min_periods=1, center=True).std() / (ear_d.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "    \n",
    "    # Add Egocentric Features\n",
    "    ego_df = compute_egocentric_transform(single_mouse, body_parts_tracked)\n",
    "    if ego_df is not None:\n",
    "        X = pd.concat([X, ego_df], axis=1)\n",
    "\n",
    "    return X.astype(np.float32, copy=False)\n",
    "\n",
    "def transform_pair(mouse_pair, body_parts_tracked, fps):\n",
    "    avail_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    avail_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "    X = pd.DataFrame({f\"12+{p1}+{p2}\": np.square(mouse_pair['A'][p1] - mouse_pair['B'][p2]).sum(axis=1, skipna=False) for p1, p2 in itertools.product(body_parts_tracked, repeat=2) if p1 in avail_A and p2 in avail_B})\n",
    "    X = X.reindex(columns=[f\"12+{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        lag = _scale(10, fps)\n",
    "        shA = mouse_pair['A']['ear_left'].shift(lag)\n",
    "        shB = mouse_pair['B']['ear_left'].shift(lag)\n",
    "        speeds = pd.DataFrame({'sp_A': np.square(mouse_pair['A']['ear_left'] - shA).sum(axis=1, skipna=False), 'sp_AB': np.square(mouse_pair['A']['ear_left'] - shB).sum(axis=1, skipna=False), 'sp_B': np.square(mouse_pair['B']['ear_left'] - shB).sum(axis=1, skipna=False)})\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n",
    "        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n",
    "        X['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (np.sqrt(dir_A['x']**2 + dir_A['y']**2) * np.sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6)\n",
    "    if all(p in avail_A for p in ['nose']) and all(p in avail_B for p in ['nose']):\n",
    "        cur = np.square(mouse_pair['A']['nose'] - mouse_pair['B']['nose']).sum(axis=1, skipna=False)\n",
    "        lag = _scale(10, fps)\n",
    "        shA_n = mouse_pair['A']['nose'].shift(lag)\n",
    "        shB_n = mouse_pair['B']['nose'].shift(lag)\n",
    "        past = np.square(shA_n - shB_n).sum(axis=1, skipna=False)\n",
    "        X['appr'] = cur - past\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd = np.sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 + (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n",
    "        X['v_cls'] = (cd < 5.0).astype(float)\n",
    "        X['cls']   = ((cd >= 5.0) & (cd < 15.0)).astype(float)\n",
    "        X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n",
    "        X['far']   = (cd >= 30.0).astype(float)\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd_full = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'd_m{w}']  = cd_full.rolling(ws, **roll).mean()\n",
    "            X[f'd_s{w}']  = cd_full.rolling(ws, **roll).std()\n",
    "            X[f'd_mn{w}'] = cd_full.rolling(ws, **roll).min()\n",
    "            X[f'd_mx{w}'] = cd_full.rolling(ws, **roll).max()\n",
    "            d_var = cd_full.rolling(ws, **roll).var()\n",
    "            X[f'int{w}'] = 1 / (1 + d_var)\n",
    "            Axd = mouse_pair['A']['body_center']['x'].diff()\n",
    "            Ayd = mouse_pair['A']['body_center']['y'].diff()\n",
    "            Bxd = mouse_pair['B']['body_center']['x'].diff()\n",
    "            Byd = mouse_pair['B']['body_center']['y'].diff()\n",
    "            coord = Axd * Bxd + Ayd * Byd\n",
    "            X[f'co_m{w}'] = coord.rolling(ws, **roll).mean()\n",
    "            X[f'co_s{w}'] = coord.rolling(ws, **roll).std()\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        nn = np.sqrt((mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 + (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nn_lg{lag}']  = nn.shift(l)\n",
    "            X[f'nn_ch{lag}']  = nn - nn.shift(l)\n",
    "            is_cl = (nn < 10.0).astype(float)\n",
    "            X[f'cl_ps{lag}']  = is_cl.rolling(l, min_periods=1).mean()\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        Avx = mouse_pair['A']['body_center']['x'].diff()\n",
    "        Avy = mouse_pair['A']['body_center']['y'].diff()\n",
    "        Bvx = mouse_pair['B']['body_center']['x'].diff()\n",
    "        Bvy = mouse_pair['B']['body_center']['y'].diff()\n",
    "        val = (Avx * Bvx + Avy * Bvy) / (np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6)\n",
    "        for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'va_{off}'] = val.shift(-o)\n",
    "        w = _scale(30, fps)\n",
    "        X['int_con'] = cd_full.rolling(w, min_periods=1, center=True).std() / (cd_full.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "        X = add_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "    \n",
    "    # Add Egocentric Features for Pair\n",
    "    # 1. A's ego features (A in A's frame)\n",
    "    ego_A = compute_egocentric_transform(mouse_pair['A'], body_parts_tracked)\n",
    "    if ego_A is not None:\n",
    "        ego_A.columns = [f\"A_{c}\" for c in ego_A.columns]\n",
    "        X = pd.concat([X, ego_A], axis=1)\n",
    "        \n",
    "    # 2. B's ego features (B in B's frame) - to know B's posture\n",
    "    ego_B = compute_egocentric_transform(mouse_pair['B'], body_parts_tracked)\n",
    "    if ego_B is not None:\n",
    "        ego_B.columns = [f\"B_{c}\" for c in ego_B.columns]\n",
    "        X = pd.concat([X, ego_B], axis=1)\n",
    "\n",
    "    # 3. B in A's frame (Interaction context: where is B relative to A?)\n",
    "    if 'body_center' in mouse_pair['B'].columns:\n",
    "        # Prepare B's key points to transform into A's frame\n",
    "        target_pts = {\n",
    "            'body_center': (mouse_pair['B']['body_center']['x'], mouse_pair['B']['body_center']['y'])\n",
    "        }\n",
    "        if 'nose' in mouse_pair['B'].columns:\n",
    "            target_pts['nose'] = (mouse_pair['B']['nose']['x'], mouse_pair['B']['nose']['y'])\n",
    "            \n",
    "        ego_B_in_A = compute_egocentric_transform(mouse_pair['A'], [], target_points=target_pts)\n",
    "        if ego_B_in_A is not None:\n",
    "             X = pd.concat([X, ego_B_in_A], axis=1)\n",
    "\n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mouse_data(dataset, traintest, traintest_directory=None, generate_single=True, generate_pair=True):\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset), desc=f\"Generating data for {traintest}\"):\n",
    "        lab_id = row.lab_id\n",
    "        if type(row.behaviors_labeled) != str and traintest == 'train' and not lab_id.startswith('MABe22'):\n",
    "             continue\n",
    "        video_id = row.video_id\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            vid = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid.query(\"~ bodypart.isin(@drop_body_parts)\")\n",
    "        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n",
    "        del vid\n",
    "        gc.collect()\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n",
    "        pvid /= row.pix_per_cm_approx\n",
    "        if type(row.behaviors_labeled) == str:\n",
    "            vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "            vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "            vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "            vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "            \n",
    "            # Remap actions to 'others1' and 'others2'\n",
    "            vid_behaviors.loc[vid_behaviors['action'].isin(CFG.OTHERS1_ACTIONS), 'action'] = 'others1'\n",
    "            vid_behaviors.loc[vid_behaviors['action'].isin(CFG.OTHERS2_ACTIONS), 'action'] = 'others2'\n",
    "        else:\n",
    "            vid_behaviors = pd.DataFrame(columns=['agent', 'target', 'action'])\n",
    "        annot = None\n",
    "        if traintest == 'train' and not lab_id.startswith('MABe22'):\n",
    "            try:\n",
    "                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        if generate_single:\n",
    "            agents = np.unique(pvid.columns.get_level_values('mouse_id'))\n",
    "            for mouse_id in agents:\n",
    "                mouse_id_str = f\"mouse{mouse_id}\"\n",
    "                try:\n",
    "                    if annot is not None:\n",
    "                        vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n",
    "                        vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "                    else:\n",
    "                        vid_agent_actions = []\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    assert len(single_mouse) == len(pvid)\n",
    "                    single_mouse_meta = pd.DataFrame({'video_id': video_id, 'agent_id': mouse_id_str, 'target_id': 'self', 'video_frame': single_mouse.index})\n",
    "                    if annot is not None:\n",
    "                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            action = annot_row.action\n",
    "                            if action in CFG.OTHERS1_ACTIONS: action = 'others1'\n",
    "                            elif action in CFG.OTHERS2_ACTIONS: action = 'others2'\n",
    "                            if action in single_mouse_label.columns:\n",
    "                                single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], action] = 1.0\n",
    "                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n",
    "                    else:\n",
    "                        yield 'single', single_mouse, single_mouse_meta, None\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        if generate_pair:\n",
    "            agents = np.unique(pvid.columns.get_level_values('mouse_id'))\n",
    "            if len(agents) >= 2:\n",
    "                for agent, target in itertools.permutations(agents, 2): \n",
    "                    agent_str = f\"mouse{agent}\"\n",
    "                    target_str = f\"mouse{target}\"\n",
    "                    if annot is not None:\n",
    "                        vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "                        vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "                    else:\n",
    "                        vid_agent_actions = []\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n",
    "                    assert len(mouse_pair) == len(pvid)\n",
    "                    mouse_pair_meta = pd.DataFrame({'video_id': video_id, 'agent_id': agent_str, 'target_id': target_str, 'video_frame': mouse_pair.index})\n",
    "                    if annot is not None:\n",
    "                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            action = annot_row.action\n",
    "                            if action in CFG.OTHERS1_ACTIONS: action = 'others1'\n",
    "                            elif action in CFG.OTHERS2_ACTIONS: action = 'others2'\n",
    "                            if action in mouse_pair_label.columns:\n",
    "                                mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], action] = 1.0\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n",
    "                    else:\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3bff0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training and Prediction Functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost(X, y, groups, model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    cv = GroupKFold(n_splits=CFG.n_splits)\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    n_pos = y.sum()\n",
    "    n_neg = len(y) - n_pos\n",
    "    scale = n_neg / (n_pos + 1)  # Minimum 10.0 as requested\n",
    "    print(f\"  Class ratio - Positive: {n_pos}, Negative: {n_neg}, Scale: {scale:.2f}\")\n",
    "    \n",
    "    oof_preds = np.zeros(len(y))\n",
    "    models = []\n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_valid, y_valid = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "        params_with_scale = CFG.xgb_params.copy()\n",
    "        params_with_scale['scale_pos_weight'] = scale\n",
    "        model = XGBClassifier(**params_with_scale)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "        \n",
    "        # Predict and Smooth\n",
    "        raw_preds = model.predict_proba(X_valid)[:, 1]\n",
    "        oof_preds[valid_idx] = apply_temporal_smoothing(raw_preds)\n",
    "        \n",
    "        models.append(model)\n",
    "    joblib.dump(models, f\"{model_dir}/xgb_models.pkl\")\n",
    "    return oof_preds, models\n",
    "\n",
    "def tune_threshold(y_true, y_pred_proba):\n",
    "    def objective(trial):\n",
    "        threshold = trial.suggest_float(\"threshold\", 0.1, 0.9, step=0.01)\n",
    "        return f1_score(y_true, (y_pred_proba >= threshold), zero_division=0)\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10, n_jobs=-1)\n",
    "    return study.best_params[\"threshold\"]\n",
    "\n",
    "def apply_temporal_smoothing(probs, window_size=15):\n",
    "    \"\"\"\n",
    "    Applies a rolling mean smoothing to the probability sequence.\n",
    "    \"\"\"\n",
    "    if len(probs) < window_size:\n",
    "        return probs\n",
    "    \n",
    "    # Use pandas rolling for convenience\n",
    "    s = pd.Series(probs)\n",
    "    # Center=True to avoid phase shift\n",
    "    smoothed = s.rolling(window=window_size, min_periods=1, center=True).mean().values\n",
    "    return smoothed\n",
    "\n",
    "def save_feature_importance(models, feature_names, output_dir, prefix):\n",
    "    \"\"\"\n",
    "    Aggregates and saves feature importance from a list of XGBoost models.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    if not models:\n",
    "        return\n",
    "        \n",
    "    importance_df = pd.DataFrame(index=feature_names)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        # Get importance type 'gain' (average gain)\n",
    "        imp = model.get_booster().get_score(importance_type='gain')\n",
    "        # Map to feature names (XGBoost might drop unused features)\n",
    "        imp_series = pd.Series(imp).reindex(feature_names, fill_value=0)\n",
    "        importance_df[f'model_{i}'] = imp_series\n",
    "        \n",
    "    importance_df['mean_importance'] = importance_df.mean(axis=1)\n",
    "    importance_df = importance_df.sort_values('mean_importance', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    importance_df.to_csv(f\"{output_dir}/feature_importance_{prefix}.csv\")\n",
    "    \n",
    "    print(f\"\\n    Top 10 Features ({prefix}):\")\n",
    "    print(importance_df['mean_importance'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mabe_f1_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MABe F1 Score Evaluation Functions\n",
    "# =============================================================================\n",
    "from collections import defaultdict\n",
    "\n",
    "def segments_to_frames(segments_df):\n",
    "    \"\"\"Convert segment DataFrame to frame-level dictionary\"\"\"\n",
    "    frame_dict = defaultdict(set)\n",
    "    \n",
    "    for _, row in segments_df.iterrows():\n",
    "        key = f\"{row['video_id']}_{row['agent_id']}_{row['target_id']}_{row['action']}\"\n",
    "        frame_dict[key].update(range(row['start_frame'], row['stop_frame']))\n",
    "    \n",
    "    return frame_dict\n",
    "\n",
    "def calculate_mabe_f1(ground_truth_df, prediction_df, beta=1.0):\n",
    "    \"\"\"Calculate MABe F1 score\"\"\"\n",
    "    \n",
    "    # Convert to frame sets\n",
    "    gt_frames = segments_to_frames(ground_truth_df)\n",
    "    pred_frames = segments_to_frames(prediction_df)\n",
    "    \n",
    "    # Calculate per-action metrics\n",
    "    tps = defaultdict(int)\n",
    "    fps = defaultdict(int)\n",
    "    fns = defaultdict(int)\n",
    "    \n",
    "    # All unique keys from both GT and predictions\n",
    "    all_keys = set(gt_frames.keys()) | set(pred_frames.keys())\n",
    "    distinct_actions = set()\n",
    "    \n",
    "    for key in all_keys:\n",
    "        action = key.split('_')[-1]\n",
    "        distinct_actions.add(action)\n",
    "        \n",
    "        gt_set = gt_frames.get(key, set())\n",
    "        pred_set = pred_frames.get(key, set())\n",
    "        \n",
    "        tps[action] += len(gt_set & pred_set)  # Intersection\n",
    "        fps[action] += len(pred_set - gt_set)  # Predicted but not in GT\n",
    "        fns[action] += len(gt_set - pred_set)  # In GT but not predicted\n",
    "    \n",
    "    # Calculate F1 per action\n",
    "    action_f1s = []\n",
    "    results_detail = {}\n",
    "    \n",
    "    for action in sorted(distinct_actions):\n",
    "        tp = tps[action]\n",
    "        fp = fps[action]\n",
    "        fn = fns[action]\n",
    "        \n",
    "        if tp + fp + fn == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = (1 + beta**2) * tp / ((1 + beta**2) * tp + beta**2 * fn + fp)\n",
    "        \n",
    "        action_f1s.append(f1)\n",
    "        results_detail[action] = {\n",
    "            'TP': tp, 'FP': fp, 'FN': fn,\n",
    "            'Precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "            'Recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "            'F1': f1\n",
    "        }\n",
    "    \n",
    "    overall_f1 = sum(action_f1s) / len(action_f1s) if action_f1s else 0.0\n",
    "    \n",
    "    return overall_f1, results_detail\n",
    "\n",
    "def probabilities_to_segments(probs, threshold, video_id, agent, target, action):\n",
    "    \"\"\"Convert probability sequence to segments\"\"\"\n",
    "    binary = (probs >= threshold).astype(int)\n",
    "    diff = np.diff(np.concatenate(([0], binary, [0])))\n",
    "    starts = np.where(diff == 1)[0]\n",
    "    stops = np.where(diff == -1)[0]\n",
    "    \n",
    "    segments = []\n",
    "    for start, stop in zip(starts, stops):\n",
    "        segments.append({\n",
    "            'video_id': video_id,\n",
    "            'agent_id': agent,\n",
    "            'target_id': target,\n",
    "            'action': action,\n",
    "            'start_frame': int(start),\n",
    "            'stop_frame': int(stop)\n",
    "        })\n",
    "    return segments\n",
    "\n",
    "print(\"✅ MABe F1 evaluation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f604b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train.csv...\n",
      "Labeled videos: 863\n",
      "Unlabeled (MABe22) videos: 7926\n",
      "  - Used for Pseudo-labeling (10%): 792\n",
      "  - Used for Validation (10%): 792\n",
      "\n",
      "--- Phase 1: Training Unified Baseline Model ---\n",
      "Canonical body parts: ['nose', 'ear_left', 'ear_right', 'body_center', 'tail_base', 'spine_middle', 'tail_tip']\n",
      "Generating features for all labeled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data for train: 100%|██████████| 863/863 [05:20<00:00,  2.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Baseline for Mode: SINGLE ---\n",
      "  Action: others1\n",
      "    OOF F1: 0.4469 (Thresh: 0.16)\n",
      "\n",
      "    Top 10 Features (phase1):\n",
      "ear_right+body_center    4524.241862\n",
      "ear_left+body_center     4024.168193\n",
      "ear_left+tail_tip        2363.867757\n",
      "tail_base+tail_tip       2026.913574\n",
      "s0_20                    1959.736453\n",
      "cx_m30                   1291.299255\n",
      "nose+ear_right           1242.225505\n",
      "ear_right+tail_tip       1026.902527\n",
      "nose+body_center          835.080048\n",
      "ego_tail_base_y           781.477132\n",
      "Name: mean_importance, dtype: float64\n",
      "  Action: others2\n",
      "    OOF F1: 0.4808 (Thresh: 0.29)\n",
      "\n",
      "    Top 10 Features (phase1):\n",
      "s0_20             2038.813141\n",
      "nose+ear_left     1742.871602\n",
      "sp_lf2            1322.224772\n",
      "sp_lf             1268.439412\n",
      "nt_lg10           1226.905314\n",
      "sp_rt             1173.466370\n",
      "nose+tail_base    1140.424723\n",
      "act5              1093.417887\n",
      "sp_m20            1003.062960\n",
      "nose+tail_tip      891.708008\n",
      "Name: mean_importance, dtype: float64\n",
      "  Action: rear\n",
      "    OOF F1: 0.3994 (Thresh: 0.15)\n",
      "\n",
      "    Top 10 Features (phase1):\n",
      "ear_right+tail_base    1881.559652\n",
      "s0_40                  1678.809611\n",
      "s0_20                  1670.795247\n",
      "ear_left+tail_base     1339.542196\n",
      "ear_left+ear_right     1276.909505\n",
      "nose+ear_right         1185.404867\n",
      "ear_o-10               1120.636943\n",
      "y_rng60                 971.514018\n",
      "ear_con                 935.077209\n",
      "sp_rt                   851.679850\n",
      "Name: mean_importance, dtype: float64\n",
      "  Action: selfgroom\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 76\u001B[0m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m     75\u001B[0m model_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCFG\u001B[38;5;241m.\u001B[39mmodel_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCFG\u001B[38;5;241m.\u001B[39mmodel_name_xgb_base\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maction\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 76\u001B[0m oof_preds, models \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_xgboost\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_tr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_action\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m best_thresh \u001B[38;5;241m=\u001B[39m tune_threshold(y_action, oof_preds)\n\u001B[1;32m     79\u001B[0m f1 \u001B[38;5;241m=\u001B[39m f1_score(y_action, (oof_preds \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m best_thresh))\n",
      "Cell \u001B[0;32mIn[7], line 14\u001B[0m, in \u001B[0;36mtrain_xgboost\u001B[0;34m(X, y, groups, model_dir)\u001B[0m\n\u001B[1;32m     12\u001B[0m X_valid, y_valid \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39miloc[valid_idx], y\u001B[38;5;241m.\u001B[39miloc[valid_idx]\n\u001B[1;32m     13\u001B[0m model \u001B[38;5;241m=\u001B[39m XGBClassifier(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mCFG\u001B[38;5;241m.\u001B[39mxgb_params)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_valid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_valid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Predict and Smooth\u001B[39;00m\n\u001B[1;32m     17\u001B[0m raw_preds \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict_proba(X_valid)[:, \u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/seoulstation/lib/python3.8/site-packages/xgboost/core.py:726\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    725\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/seoulstation/lib/python3.8/site-packages/xgboost/sklearn.py:1531\u001B[0m, in \u001B[0;36mXGBClassifier.fit\u001B[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[0m\n\u001B[1;32m   1511\u001B[0m model, metric, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_fit(xgb_model, params)\n\u001B[1;32m   1512\u001B[0m train_dmatrix, evals \u001B[38;5;241m=\u001B[39m _wrap_evaluation_matrices(\n\u001B[1;32m   1513\u001B[0m     missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing,\n\u001B[1;32m   1514\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1528\u001B[0m     feature_types\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_types,\n\u001B[1;32m   1529\u001B[0m )\n\u001B[0;32m-> 1531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective):\n\u001B[1;32m   1546\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/seoulstation/lib/python3.8/site-packages/xgboost/core.py:726\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    725\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/seoulstation/lib/python3.8/site-packages/xgboost/training.py:182\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[0m\n\u001B[1;32m    180\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    181\u001B[0m     bst\u001B[38;5;241m.\u001B[39mupdate(dtrain, iteration\u001B[38;5;241m=\u001B[39mi, fobj\u001B[38;5;241m=\u001B[39mobj)\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcb_container\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mafter_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevals\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    183\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    185\u001B[0m bst \u001B[38;5;241m=\u001B[39m cb_container\u001B[38;5;241m.\u001B[39mafter_training(bst)\n",
      "File \u001B[0;32m~/miniconda3/envs/seoulstation/lib/python3.8/site-packages/xgboost/callback.py:258\u001B[0m, in \u001B[0;36mCallbackContainer.after_iteration\u001B[0;34m(self, model, epoch, dtrain, evals)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, name \u001B[38;5;129;01min\u001B[39;00m evals:\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m name\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset name should not contain `-`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 258\u001B[0m score: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_set\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_output_margin\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    259\u001B[0m metric_score \u001B[38;5;241m=\u001B[39m _parse_eval_str(score)\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_history(metric_score, epoch)\n",
      "File \u001B[0;32m~/miniconda3/envs/seoulstation/lib/python3.8/site-packages/xgboost/core.py:2212\u001B[0m, in \u001B[0;36mBooster.eval_set\u001B[0;34m(self, evals, iteration, feval, output_margin)\u001B[0m\n\u001B[1;32m   2209\u001B[0m evnames \u001B[38;5;241m=\u001B[39m c_array(ctypes\u001B[38;5;241m.\u001B[39mc_char_p, [c_str(d[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m evals])\n\u001B[1;32m   2210\u001B[0m msg \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mc_char_p()\n\u001B[1;32m   2211\u001B[0m _check_call(\n\u001B[0;32m-> 2212\u001B[0m     \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterEvalOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2213\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2215\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdmats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mevnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2217\u001B[0m \u001B[43m        \u001B[49m\u001B[43mc_bst_ulong\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2218\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2219\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2220\u001B[0m )\n\u001B[1;32m   2221\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m msg\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2222\u001B[0m res \u001B[38;5;241m=\u001B[39m msg\u001B[38;5;241m.\u001B[39mvalue\u001B[38;5;241m.\u001B[39mdecode()  \u001B[38;5;66;03m# pylint: disable=no-member\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Main Execution (Optimized + Smart Subsampling)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # [NEW] Global OOF Storage for Final Evaluation\n",
    "    global_oof_preds = []\n",
    "    global_ground_truth = []\n",
    "    # =========================================================================\n",
    "    # Phase 0: Setup\n",
    "    # =========================================================================\n",
    "    print(\"Loading train.csv...\")\n",
    "    train = pd.read_csv(CFG.train_path)\n",
    "    train_labeled = train[~train.lab_id.str.startswith('MABe22')]\n",
    "    train_unlabeled = train[train.lab_id.str.startswith('MABe22')]\n",
    "    \n",
    "    # Shuffle and split MABe22 data\n",
    "    train_unlabeled = train_unlabeled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n_unlabeled = len(train_unlabeled)\n",
    "    n_subset = int(n_unlabeled * 0.1)\n",
    "    \n",
    "    train_unlabeled_train = train_unlabeled.iloc[:n_subset]\n",
    "    train_unlabeled_val = train_unlabeled.iloc[n_subset:n_subset*2]\n",
    "    \n",
    "    print(f\"Labeled videos: {len(train_labeled)}\")\n",
    "    print(f\"Unlabeled (MABe22) videos: {len(train_unlabeled)}\")\n",
    "    print(f\"  - Used for Pseudo-labeling (10%): {len(train_unlabeled_train)}\")\n",
    "    print(f\"  - Used for Validation (10%): {len(train_unlabeled_val)}\")\n",
    "    \n",
    "    _fps_lookup = train_labeled[['video_id', 'frames_per_second']].drop_duplicates('video_id').set_index('video_id')['frames_per_second'].to_dict()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Sequential Processing: Single -> Clear Memory -> Pair\n",
    "    # =========================================================================\n",
    "    for mode in ['single', 'pair']:\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\" PROCESSING MODE: {mode.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # --- 1. Generate Labeled Data for Current Mode ---\n",
    "        print(f\"Generating labeled data for {mode}...\")\n",
    "        gen_labeled = generate_mouse_data(\n",
    "            train_labeled, 'train', \n",
    "            generate_single=(mode=='single'), \n",
    "            generate_pair=(mode=='pair')\n",
    "        )\n",
    "        \n",
    "        data_storage = {'X': [], 'y': [], 'meta': []}\n",
    "        all_actions = set()\n",
    "        \n",
    "        for switch, data, meta, label in gen_labeled:\n",
    "            if switch != mode: continue\n",
    "            if label is None or label.shape[1] == 0: continue\n",
    "            \n",
    "            fps_i = _fps_from_meta(meta, _fps_lookup, default_fps=30.0)\n",
    "            if mode == 'single':\n",
    "                X_i = transform_single(data, CFG.CANONICAL_BODY_PARTS, fps_i)\n",
    "            else:\n",
    "                X_i = transform_pair(data, CFG.CANONICAL_BODY_PARTS, fps_i)\n",
    "            \n",
    "            # --- SMART SUBSAMPLING (Both Modes) ---\n",
    "            if mode in ['single', 'pair']:\n",
    "                # Align meta index with data index to ensure loc works correctly\n",
    "                meta.index = X_i.index\n",
    "                \n",
    "                # Identify active frames (any behavior present)\n",
    "                active_mask = label.sum(axis=1) > 0\n",
    "                bg_mask = ~active_mask\n",
    "                \n",
    "                # Keep 100% of active frames\n",
    "                active_indices = label[active_mask].index\n",
    "                \n",
    "                # Sample background frames (3:1 ratio with active)\n",
    "                if bg_mask.sum() > 0:\n",
    "                    if len(active_indices) > 0:\n",
    "                        sample_size = min(len(active_indices) * 1, bg_mask.sum())\n",
    "                    else:\n",
    "                        sample_size = min(1000, bg_mask.sum())\n",
    "                    bg_indices = label[bg_mask].sample(n=sample_size, random_state=42).index\n",
    "                else:\n",
    "                    bg_indices = pd.Index([])\n",
    "                \n",
    "                # Combine and sort indices\n",
    "                keep_indices = active_indices.union(bg_indices).sort_values()\n",
    "                \n",
    "                # Filter data\n",
    "                X_i = X_i.loc[keep_indices]\n",
    "                X_i = X_i.astype(np.float32) # [NEW] Reduce memory\n",
    "                X_i = X_i.astype(np.float32) # [NEW] Reduce memory\n",
    "                label = label.loc[keep_indices]\n",
    "                meta = meta.loc[keep_indices]\n",
    "                \n",
    "                # Log data statistics\n",
    "                print(f\"    Video {meta['video_id'].iloc[0]}: Active={len(active_indices)}, Background={len(bg_indices)}, Ratio={len(active_indices)/(len(bg_indices)+1):.2f}\")\n",
    "                \n",
    "            data_storage['X'].append(X_i)\n",
    "            data_storage['y'].append(label)\n",
    "            data_storage['meta'].append(meta)\n",
    "            all_actions.update(label.columns)\n",
    "            del data, meta, label, X_i\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        if not data_storage['X']:\n",
    "            print(f\"No data found for mode {mode}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        X_tr = pd.concat(data_storage['X'], axis=0, ignore_index=True)\n",
    "        y_tr_df = pd.concat(data_storage['y'], axis=0, ignore_index=True).fillna(0.0)\n",
    "        meta_tr = pd.concat(data_storage['meta'], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Clear list storage immediately\n",
    "        del data_storage\n",
    "        gc.collect()\n",
    "        \n",
    "        # --- 2. Phase 1 Training ---\n",
    "        print(f\"\\n--- Phase 1: Training Baseline ({mode}) ---\")\n",
    "        actions_in_mode = sorted([a for a in all_actions if a in y_tr_df.columns])\n",
    "        y_tr_df = y_tr_df.reindex(columns=actions_in_mode, fill_value=0.0)\n",
    "        \n",
    "        phase1_models_mode = {}\n",
    "        phase1_thresholds_mode = {}\n",
    "        \n",
    "        for action in actions_in_mode:\n",
    "            print(f\"  Action: {action}\")\n",
    "            y_action = y_tr_df[action].values\n",
    "            groups = meta_tr.video_id\n",
    "            if y_action.sum() < CFG.n_splits:\n",
    "                print(f\"    Skipping (not enough positive samples)\")\n",
    "                continue\n",
    "                \n",
    "            model_dir = f\"{CFG.model_path}/{CFG.model_name_xgb_base}/{mode}/{action}\"\n",
    "            oof_preds, models = train_xgboost(X_tr, pd.Series(y_action), groups, model_dir)\n",
    "            best_thresh = tune_threshold(y_action, oof_preds)\n",
    "            \n",
    "            f1 = f1_score(y_action, (oof_preds >= best_thresh))\n",
    "            print(f\"    OOF F1: {f1:.4f} (Thresh: {best_thresh:.2f})\")\n",
    "            \n",
    "            phase1_models_mode[action] = models\n",
    "            phase1_thresholds_mode[action] = best_thresh\n",
    "            joblib.dump(best_thresh, f\"{model_dir}/threshold.pkl\")\n",
    "            save_feature_importance(models, X_tr.columns, model_dir, \"phase1\")\n",
    "\n",
    "            # [NEW] Collect OOF predictions for Global Evaluation\n",
    "            # Reconstruct segments from OOF frame predictions\n",
    "            # We need to map back to original video_id and frames\n",
    "            # Since X_tr is concatenated, we iterate through groups\n",
    "            \n",
    "            # Create a temporary dataframe for OOF processing\n",
    "            oof_df = pd.DataFrame({\n",
    "                'video_id': meta_tr['video_id'],\n",
    "                'agent_id': meta_tr['agent_id'],\n",
    "                'target_id': meta_tr['target_id'],\n",
    "                'frame': meta_tr['video_frame'],\n",
    "                'pred': oof_preds,\n",
    "                'label': y_action\n",
    "            })\n",
    "            \n",
    "            # Process each video to extract segments\n",
    "            for vid, grp in oof_df.groupby('video_id'):\n",
    "                grp = grp.sort_values('frame')\n",
    "                agent_id = grp['agent_id'].iloc[0]\n",
    "                target_id = grp['target_id'].iloc[0]\n",
    "                \n",
    "                # Predictions to segments\n",
    "                pred_segments = probabilities_to_segments(\n",
    "                    grp['pred'].values, best_thresh, vid, agent_id, target_id, action\n",
    "                )\n",
    "                global_oof_preds.extend(pred_segments)\n",
    "                \n",
    "                # Ground Truth to segments\n",
    "                gt_binary = grp['label'].values.astype(int)\n",
    "                diff = np.diff(np.concatenate(([0], gt_binary, [0])))\n",
    "                starts = np.where(diff == 1)[0]\n",
    "                stops = np.where(diff == -1)[0]\n",
    "                \n",
    "                for start, stop in zip(starts, stops):\n",
    "                    # Map back to original frame numbers if needed, but here we use relative index\n",
    "                    # Note: meta_tr['video_frame'] contains actual frame numbers\n",
    "                    real_start = grp['frame'].iloc[start]\n",
    "                    real_stop = grp['frame'].iloc[stop-1] + 1\n",
    "                    \n",
    "                    global_ground_truth.append({\n",
    "                        'video_id': vid,\n",
    "                        'agent_id': agent_id,\n",
    "                        'target_id': target_id,\n",
    "                        'action': action,\n",
    "                        'start_frame': real_start,\n",
    "                        'stop_frame': real_stop\n",
    "                    })\n",
    "            \n",
    "            del oof_preds, models, y_action\n",
    "            gc.collect()\n",
    "            \n",
    "        # --- Phase 2: Pseudo-Labeling (REMOVED in v7) ---\n",
    "        print(f\"Cleaning up memory for mode {mode}...\")\n",
    "        del X_tr, y_tr_df, meta_tr\n",
    "        if 'phase1_models_mode' in locals(): del phase1_models_mode\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    # Global F1 Score Calculation\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" GLOBAL F1 SCORE REPORT\")\n",
    "    if global_oof_preds and global_ground_truth:\n",
    "        print(\"Calculating Global MABe F1 Score from OOF predictions...\")\n",
    "        oof_pred_df = pd.DataFrame(global_oof_preds)\n",
    "        oof_gt_df = pd.DataFrame(global_ground_truth)\n",
    "        \n",
    "        global_f1, global_details = calculate_mabe_f1(oof_gt_df, oof_pred_df)\n",
    "        \n",
    "        # [NEW] Calculate Single vs Pair F1\n",
    "        single_actions = set(CFG.OTHERS1_ACTIONS + CFG.OTHERS2_ACTIONS + ['rear', 'selfgroom', 'dig'])\n",
    "        pair_actions = set(['investigate', 'mount', 'attack', 'chase', 'sniff', 'sniffgenital', 'sniffbody', 'sniffface', 'approach', 'escape', 'avoid', 'submit', 'intromit', 'chaseattack'])\n",
    "        \n",
    "        single_f1s = []\n",
    "        pair_f1s = []\n",
    "        \n",
    "        for action, d in global_details.items():\n",
    "            # Simple heuristic: if action is in our known single lists, it's single. Else pair.\n",
    "            # Or check if it was trained in 'single' mode loop. But here we just use names.\n",
    "            # Note: 'others1' and 'others2' can be in both, but usually single.\n",
    "            # Let's use the 'mode' if we had stored it, but we didn't.\n",
    "            # Let's just print all and let user see.\n",
    "            pass\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" GLOBAL CV F1 SCORE: {global_f1:.4f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(\"\\nPer-Action CV Results:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Action':<20} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Sort by F1 score descending\n",
    "        sorted_actions = sorted(global_details.keys(), key=lambda x: global_details[x]['F1'], reverse=True)\n",
    "        \n",
    "        for action in sorted_actions:\n",
    "            d = global_details[action]\n",
    "            print(f\"{action:<20} {d['Precision']:<12.4f} {d['Recall']:<12.4f} {d['F1']:<12.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Note: This requires saving OOF predictions during training\n",
    "    # For now, we show per-action F1 scores that were already printed\n",
    "    print(\"\\nSee individual action F1 scores above.\")\n",
    "    \n",
    "    print(\"\\nDone! All modes processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Phase 4: Inference & Submission Generation\n",
    "# =============================================================================\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# [NEW] Post-processing: Gap Filling\n",
    "def fill_gaps(binary_preds, max_gap=5):\n",
    "    \"\"\"Fills short gaps (0s) between 1s.\"\"\"\n",
    "    # Convert to int for processing\n",
    "    preds = binary_preds.astype(int)\n",
    "    # Find runs of 0s\n",
    "    # Pad with 1s to handle edge cases\n",
    "    padded = np.concatenate(([1], preds, [1]))\n",
    "    diff = np.diff(padded)\n",
    "    starts = np.where(diff == -1)[0]\n",
    "    stops = np.where(diff == 1)[0]\n",
    "    \n",
    "    for start, stop in zip(starts, stops):\n",
    "        gap_len = stop - start\n",
    "        if gap_len <= max_gap:\n",
    "            preds[start:stop] = 1\n",
    "    return preds\n",
    "\n",
    "# [NEW] Post-processing: Min Duration\n",
    "def remove_short_duration(binary_preds, min_len=5):\n",
    "    \"\"\"Removes short bursts of 1s.\"\"\"\n",
    "    preds = binary_preds.astype(int)\n",
    "    padded = np.concatenate(([0], preds, [0]))\n",
    "    diff = np.diff(padded)\n",
    "    starts = np.where(diff == 1)[0]\n",
    "    stops = np.where(diff == -1)[0]\n",
    "    \n",
    "    for start, stop in zip(starts, stops):\n",
    "        duration = stop - start\n",
    "        if duration <= min_len:\n",
    "            preds[start:stop] = 0\n",
    "    return preds\n",
    "\n",
    "# [Helper Function] 확률 -> 구간(Segment) 변환\n",
    "def probabilities_to_segments(probs, threshold, video_id, agent, target, action):\n",
    "    binary = (probs >= threshold).astype(int)\n",
    "    # 0->1 (시작), 1->0 (끝) 지점 찾기\n",
    "    diff = np.diff(np.concatenate(([0], binary, [0])))\n",
    "    starts = np.where(diff == 1)[0]\n",
    "    stops = np.where(diff == -1)[0]\n",
    "    \n",
    "    segments = []\n",
    "    for start, stop in zip(starts, stops):\n",
    "        segments.append({\n",
    "            'video_id': video_id,\n",
    "            'agent_id': agent,\n",
    "            'target_id': target,\n",
    "            'action': action,\n",
    "            'start_frame': start,\n",
    "            'stop_frame': stop\n",
    "        })\n",
    "    return segments\n",
    "\n",
    "submission_data = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" PHASE 4: INFERENCE & SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Single / Pair 모드 각각에 대해 테스트 데이터 예측 수행\n",
    "for mode in ['single', 'pair']:\n",
    "    print(f\"\\n--- Inference for mode: {mode} ---\")\n",
    "    \n",
    "    try:\n",
    "        # 테스트 데이터 로드\n",
    "        test = pd.read_csv(CFG.test_path)\n",
    "        \n",
    "        # 테스트 데이터 제너레이터 생성\n",
    "        gen_test = generate_mouse_data(\n",
    "            test, 'test', \n",
    "            generate_single=(mode=='single'), \n",
    "            generate_pair=(mode=='pair')\n",
    "        )\n",
    "        \n",
    "        for switch, data_t, meta_t, _ in gen_test:\n",
    "            if switch != mode: continue\n",
    "            \n",
    "            # 피처 엔지니어링\n",
    "            fps_t = _fps_from_meta(meta_t, {}, default_fps=30.0)\n",
    "            if mode == 'single':\n",
    "                X_t = transform_single(data_t, CFG.CANONICAL_BODY_PARTS, fps_t)\n",
    "            else:\n",
    "                X_t = transform_pair(data_t, CFG.CANONICAL_BODY_PARTS, fps_t)\n",
    "                \n",
    "            # 저장된 모델 폴더를 찾아서 예측할 행동 목록 확인\n",
    "            # (Phase 2 모델이 있으면 우선 사용, 없으면 Phase 1 사용)\n",
    "            model_base_path = f\"{CFG.model_path}/{CFG.model_name_xgb_base}/{mode}\"\n",
    "            \n",
    "            if not os.path.exists(model_base_path):\n",
    "                print(f\"  No trained models found for {mode}. Skipping.\")\n",
    "                continue\n",
    "                \n",
    "            trained_actions = [d for d in os.listdir(model_base_path) if os.path.isdir(os.path.join(model_base_path, d))]\n",
    "            \n",
    "            # 각 행동별로 예측 수행\n",
    "            for action in trained_actions:\n",
    "                model_dir_p1 = f\"{CFG.model_path}/{CFG.model_name_xgb_base}/{mode}/{action}\"\n",
    "                \n",
    "                final_models = []\n",
    "                final_thresh = 0.5\n",
    "                \n",
    "                # 모델 로드\n",
    "                if os.path.exists(f\"{model_dir_p1}/xgb_models.pkl\"):\n",
    "                    final_models = joblib.load(f\"{model_dir_p1}/xgb_models.pkl\")\n",
    "                    final_thresh = joblib.load(f\"{model_dir_p1}/threshold.pkl\")\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                # 예측 (Ensemble Average)\n",
    "                prob = np.mean([m.predict_proba(X_t)[:, 1] for m in final_models], axis=0)\n",
    "                \n",
    "                # 후처리 (Temporal Smoothing)\n",
    "                prob = apply_temporal_smoothing(prob)\n",
    "                \n",
    "                # 결과 변환 (Segments)\n",
    "                video_id = meta_t['video_id'].iloc[0]\n",
    "                agent_id = meta_t['agent_id'].iloc[0]\n",
    "                target_id = meta_t['target_id'].iloc[0]\n",
    "                \n",
    "                \n",
    "                # [NEW] Apply Post-processing (Gap Filling + Min Duration)\n",
    "                binary_preds = (prob >= final_thresh).astype(int)\n",
    "                binary_preds = fill_gaps(binary_preds, max_gap=5)\n",
    "                binary_preds = remove_short_duration(binary_preds, min_len=5)\n",
    "                \n",
    "                # Pass processed binary preds with threshold 0.5\n",
    "                segments = probabilities_to_segments(binary_preds, 0.5, video_id, agent_id, target_id, action)\n",
    "                submission_data.extend(segments)\n",
    "                \n",
    "            del X_t, data_t, meta_t\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference for {mode}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# CSV 파일 생성\n",
    "if submission_data:\n",
    "    print(\"\\nCreating submission.csv...\")\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df['row_id'] = range(len(submission_df))\n",
    "    # 컬럼 순서 정렬\n",
    "    submission_df = submission_df[['row_id', 'video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']]\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"Submission saved to submission.csv with {len(submission_df)} segments.\")\n",
    "else:\n",
    "    print(\"\\nNo submission data generated.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b543ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fcc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376556cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c0c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae6486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Validation Evaluation on MABe22 Split\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" VALIDATION: MABe F1 Score Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "validation_predictions = []\n",
    "validation_ground_truth = []\n",
    "\n",
    "print(f\"\\nGenerating predictions on {len(train_unlabeled_val)} validation videos...\")\n",
    "\n",
    "for mode in ['single', 'pair']:\n",
    "    print(f\"\\n--- Mode: {mode} ---\")\n",
    "    \n",
    "    # Check if models exist\n",
    "    model_base_path = f\"{CFG.model_path}/{CFG.model_name_xgb_base}/{mode}\"\n",
    "    if not os.path.exists(model_base_path):\n",
    "        print(f\"  No models found for {mode}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    trained_actions = [d for d in os.listdir(model_base_path) if os.path.isdir(os.path.join(model_base_path, d))]\n",
    "    print(f\"  Trained actions: {', '.join(trained_actions)}\")\n",
    "    \n",
    "    # Generate data\n",
    "    gen_val = generate_mouse_data(\n",
    "        train_unlabeled_val.head(10),  # Sample 10 videos for quick eval\n",
    "        'train',\n",
    "        generate_single=(mode=='single'),\n",
    "        generate_pair=(mode=='pair')\n",
    "    )\n",
    "    \n",
    "    for switch, data_v, meta_v, label_v in gen_val:\n",
    "        if switch != mode: continue\n",
    "        \n",
    "        # Feature engineering\n",
    "        fps_v = _fps_from_meta(meta_v, _fps_lookup, default_fps=30.0)\n",
    "        if mode == 'single':\n",
    "            X_v = transform_single(data_v, CFG.CANONICAL_BODY_PARTS, fps_v)\n",
    "        else:\n",
    "            X_v = transform_pair(data_v, CFG.CANONICAL_BODY_PARTS, fps_v)\n",
    "        \n",
    "        video_id = meta_v['video_id'].iloc[0]\n",
    "        agent_id = meta_v['agent_id'].iloc[0]\n",
    "        target_id = meta_v['target_id'].iloc[0]\n",
    "        \n",
    "        # Predict with each action's model\n",
    "        for action in trained_actions:\n",
    "            model_dir = f\"{model_base_path}/{action}\"\n",
    "            model_path = f\"{model_dir}/xgb_models.pkl\"\n",
    "            threshold_path = f\"{model_dir}/threshold.pkl\"\n",
    "            \n",
    "            if not os.path.exists(model_path) or not os.path.exists(threshold_path):\n",
    "                continue\n",
    "            \n",
    "            models = joblib.load(model_path)\n",
    "            threshold = joblib.load(threshold_path)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            prob = np.mean([m.predict_proba(X_v)[:, 1] for m in models], axis=0)\n",
    "            prob = apply_temporal_smoothing(prob)\n",
    "            \n",
    "            # Convert to segments\n",
    "            segments = probabilities_to_segments(prob, threshold, video_id, agent_id, target_id, action)\n",
    "            validation_predictions.extend(segments)\n",
    "        \n",
    "        # Ground truth (if available)\n",
    "        if label_v is not None:\n",
    "            for action_col in label_v.columns:\n",
    "                frames_with_action = label_v[label_v[action_col] > 0].index.tolist()\n",
    "                if frames_with_action:\n",
    "                    # Group consecutive frames into segments\n",
    "                    segments = []\n",
    "                    start = frames_with_action[0]\n",
    "                    for i in range(1, len(frames_with_action)):\n",
    "                        if frames_with_action[i] != frames_with_action[i-1] + 1:\n",
    "                            segments.append({'start_frame': start, 'stop_frame': frames_with_action[i-1] + 1})\n",
    "                            start = frames_with_action[i]\n",
    "                    segments.append({'start_frame': start, 'stop_frame': frames_with_action[-1] + 1})\n",
    "                    \n",
    "                    for seg in segments:\n",
    "                        validation_ground_truth.append({\n",
    "                            'video_id': video_id,\n",
    "                            'agent_id': agent_id,\n",
    "                            'target_id': target_id,\n",
    "                            'action': action_col,\n",
    "                            'start_frame': seg['start_frame'],\n",
    "                            'stop_frame': seg['stop_frame']\n",
    "                        })\n",
    "        \n",
    "        del X_v, data_v, meta_v\n",
    "        gc.collect()\n",
    "\n",
    "# Calculate F1 score\n",
    "if validation_predictions and validation_ground_truth:\n",
    "    pred_df = pd.DataFrame(validation_predictions)\n",
    "    gt_df = pd.DataFrame(validation_ground_truth)\n",
    "    \n",
    "    overall_f1, action_details = calculate_mabe_f1(gt_df, pred_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" Overall F1 Score: {overall_f1:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nPer-Action Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Action':<20} {'Precision':<12} {'Recall':<12} {'F1':<12} {'TP':<8} {'FP':<8} {'FN':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for action in sorted(action_details.keys()):\n",
    "        d = action_details[action]\n",
    "        print(f\"{action:<20} {d['Precision']:<12.4f} {d['Recall']:<12.4f} {d['F1']:<12.4f} \"\n",
    "              f\"{d['TP']:<8} {d['FP']:<8} {d['FN']:<8}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"\\n⚠️ No predictions or ground truth available for evaluation\")\n",
    "\n",
    "print(\"\\n✅ Validation evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc847f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68853858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c78b29c3283f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Prediction Summary\n",
    "# =============================================================================\n",
    "if 'submission_df' in locals() and not submission_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" PREDICTION DISTRIBUTION (MABe22 Test Set)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count segments per action\n",
    "    action_counts = submission_df['action'].value_counts()\n",
    "    \n",
    "    print(f\"Total Predicted Segments: {len(submission_df)}\")\n",
    "    print(\"\\nAction Counts:\")\n",
    "    print(action_counts)\n",
    "    \n",
    "    print(\"\\nAction Proportions (%):\")\n",
    "    print((action_counts / len(submission_df) * 100).round(2))\n",
    "    \n",
    "    # Optional: Plot if matplotlib is available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        action_counts.plot(kind='bar')\n",
    "        plt.title('Predicted Action Distribution on MABe22 Test Set')\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No submission data found to summarize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67e6c9369999f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Prediction Summary\n",
    "# =============================================================================\n",
    "if 'submission_df' in locals() and not submission_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" PREDICTION DISTRIBUTION (MABe22 Test Set)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Count segments per action\n",
    "    action_counts = submission_df['action'].value_counts()\n",
    "    \n",
    "    print(f\"Total Predicted Segments: {len(submission_df)}\")\n",
    "    print(\"\\nAction Counts:\")\n",
    "    print(action_counts)\n",
    "    \n",
    "    print(\"\\nAction Proportions (%):\")\n",
    "    print((action_counts / len(submission_df) * 100).round(2))\n",
    "    \n",
    "    # Optional: Plot if matplotlib is available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        action_counts.plot(kind='bar')\n",
    "        plt.title('Predicted Action Distribution on MABe22 Test Set')\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No submission data found to summarize.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seoulstation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
